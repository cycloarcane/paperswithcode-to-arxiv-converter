# Example Academic Paper Collection

This is an example markdown file showing various ways Papers with Code links might appear in academic documentation.

## Research Papers

### Natural Language Processing

- **Attention Is All You Need**
  **Paper**: [Link](https://paperswithcode.com/paper/attention-is-all-you-need)  
  **Summary**: This paper introduces the Transformer architecture, a novel neural network architecture based solely on attention mechanisms.
  **Tags**: #transformer #attention #nlp

- **BERT: Pre-training of Deep Bidirectional Transformers**
  **Paper**: [Link](https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional)
  **Summary**: BERT represents a significant advancement in pre-trained language representations.

### Computer Vision

- **ResNet Paper**
  See the original ResNet paper here: https://paperswithcode.com/paper/deep-residual-learning-for-image-recognition
  This was a breakthrough in deep learning for computer vision.

### Mixed Format Examples

Some papers might be referenced inline like this [Transformer paper](https://paperswithcode.com/paper/attention-is-all-you-need) or in parentheses (https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional).

### Edge Cases

- Papers with cs.paperswithcode.com domain: https://cs.paperswithcode.com/paper/some-cybersecurity-paper
- Papers with query parameters: https://paperswithcode.com/paper/example-paper?tab=results
- Papers with fragments: https://paperswithcode.com/paper/example-paper#abstract

## References

1. Vaswani, A., et al. "Attention is all you need." [Link](https://paperswithcode.com/paper/attention-is-all-you-need)
2. Devlin, J., et al. "BERT: Pre-training of Deep Bidirectional Transformers" [Link](https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional)